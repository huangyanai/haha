package cn.spark.study.core

import org.apache.spark.SparkContext
import org.apache.spark.SparkConf

object WordCount2 {
  def main(args:Array[String]){
    val conf=new SparkConf()
             .setAppName("WordCount")
             .setMaster("local")   
    val sc=new SparkContext(conf)
    val lines=sc.textFile("F://spark/bigbigworld.txt",2)
    //根据初始RDD(lines),调用flatMap算子分隔出每一行的单词，保存在变量words中
    val words=lines.flatMap(line => line.split(" "))
    //将单个单词变成键值对，每个单词计数为1,即变成(word,1)的形式，用map算子实现
    val pairs=words.map(word=>(word,1))
    //将单个单词按照key值(word)进行计算
    val wordCounts=pairs.reduceByKey(_+_)
    //降序排序
    val sorted=wordCount.sortBy(f._2,false,2)
    //第五步，输出统计结果
    sorted.foreach{
      wordCount=>println(wordCount._1+": "+wordCount._2)
      
    }
  }
}